{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camilus77/Maritime-Vessel-Detection/blob/main/classification_of_ship_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Vision Hackathon\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "Ship or vessel detection has a wide range of applications, in the areas of maritime safety (Rosnani et al., 2022),  fisheries management, marine pollution, defence and maritime security, protection from piracy, illegal migration, etc (Agastia, 2021; Buegar et al., 2019).\n",
        "\n",
        "Keeping this in mind, a Governmental Maritime and Coastguard Agency is planning to deploy a computer vision based automated system to identify ship type only from the images taken by the survey boats. You have been hired as a consultant to build an efficient model for this project.\n",
        "\n",
        "#### Dataset Description\n",
        "There are 6252 images in train and 2680 images in test data. The categories of ships and their corresponding codes in the dataset are as follows -\n",
        "\n",
        "There are 5 classes of ships to be detected which are as follows: <br>\n",
        "\n",
        "1: Cargo <br>\n",
        "2: Military <br>\n",
        "3: Carrier <br>\n",
        "4: Cruise <br>\n",
        "5: Tankers\n",
        "\n",
        "Variable\t| Definition\n",
        "--- | ---\n",
        "image\t| Name of the image in the dataset (ID column)\n",
        "category |\tShip category code\n",
        "<br>\n",
        "#### Evaluation Metric\n",
        "The Evaluation metric for this competition is weighted F1 Score.\n",
        "\n",
        "#### Misc Rules\n",
        "- Use of external dataset is not allowed, however, transfer learning can be used to build the solution"
      ],
      "metadata": {
        "id": "n7-HZ4fLWptc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries\n"
      ],
      "metadata": {
        "id": "0VTWAGESGXQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import seed\n",
        "seed(2019)\n",
        "\n",
        "\n",
        "#visualization\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import cv2\n",
        "\n",
        "#sklearn and machine learning\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.utils import load_img, img_to_array, array_to_img\n",
        "from keras import callbacks\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "tensorflow.random.set_seed(2019) \n",
        "#avoid ugly warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#mounr drive for colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "VsEQONjCWptr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "CLjgG-6SWptv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the path for loading .jpg images\n",
        "path = \"/content/drive/MyDrive/ship_classifer_Dataset train/images\"\n",
        "\n",
        "loc = '/content/drive/MyDrive/ship_classifer_Dataset train/train.csv'\n",
        "#train_files = pd.read_html(url, dtype={'image': 'object', 'category': 'int8'})\n",
        "\n",
        "train_files = pd.read_csv(loc)#, dtype={'image': 'object', 'category': 'int8'})\n",
        "                       \n",
        "test_files = pd.read_csv('/content/drive/MyDrive/ship_classifer_Dataset train/test_ApKoW4T.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "CBIHJbx7Wptx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets have a peek at the training and testing dataset\n",
        "print(train_files.head())\n",
        "print(test_files.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "3lqqMNk2Wpty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe0b4e9-4a07-4570-87b9-2b932bc050b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         image  category\n",
            "0  2823080.jpg         1\n",
            "1  2870024.jpg         1\n",
            "2  2662125.jpg         2\n",
            "3  2900420.jpg         3\n",
            "4  2804883.jpg         2\n",
            "         image\n",
            "0  1007700.jpg\n",
            "1  1011369.jpg\n",
            "2  1051155.jpg\n",
            "3  1062001.jpg\n",
            "4  1069397.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "fNfkwn4CWpt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing values."
      ],
      "metadata": {
        "id": "U1TtcJDvWpt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us create an encoding dictionary for our ship\n"
      ],
      "metadata": {
        "id": "DrzkfijseB-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding dictionary \n",
        "ship = {'Cargo': 1, \n",
        "        'Military': 2, \n",
        "        'Carrier': 3, \n",
        "        'Cruise': 4, \n",
        "        'Tankers': 5}\n",
        "\n",
        "# let us now reverse the ship type dictionary and create our test labels (to aid interpretability)\n",
        "ship = dict([[v,k] for k,v in ship.items()])\n",
        "\n",
        "train_files['ship'] = train_files['category'].map(ship).astype('category')\n",
        "labels = list(train_files['ship'].unique())"
      ],
      "metadata": {
        "trusted": true,
        "id": "L8MSiR3lWpt8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets get visual knowledge of the number of ships in our dataset"
      ],
      "metadata": {
        "id": "dB0vlwElfUYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Ship Type Count')\n",
        "sns.countplot(y=train_files['ship'].values)\n",
        "plt.show()\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "0wmq6JQKWpuC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "a7b308b5-c6fb-474e-de09-6a17e8ce9252"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2638"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now check for inbalance in our dataset. Remember that a skewed dataset produces biased results"
      ],
      "metadata": {
        "id": "JG_dFiG2f5x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Unnormalized\n",
        "train_files['ship'].value_counts(normalize=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "peMA9XrAWpuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dcda2d2-0e25-42a7-b3ec-02ac919b7ce4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cargo       2120\n",
              "Tankers     1217\n",
              "Military    1167\n",
              "Carrier      916\n",
              "Cruise       832\n",
              "Name: ship, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalized\n",
        "train_files['ship'].value_counts(normalize=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NycCFtOcWpuG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7633409b-17b3-4f53-ba87-ae825518d5a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cargo       0.339091\n",
              "Tankers     0.194658\n",
              "Military    0.186660\n",
              "Carrier     0.146513\n",
              "Cruise      0.133077\n",
              "Name: ship, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are class imbalances in training set, we will need to display a confusion matrix visualize to where the neural network classifier is having trouble. We will also stratify train_test_split in order to maintain the class distributions in both train and test."
      ],
      "metadata": {
        "id": "vwpIXfWMWpuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode Train Labels"
      ],
      "metadata": {
        "id": "V8kXXvMgWpuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder(dtype='int8', sparse=False)\n",
        "y_train = ohe.fit_transform(train_files['category'].values.reshape(-1,1))"
      ],
      "metadata": {
        "trusted": true,
        "id": "e8InIi-hWpuO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Train/Test Files"
      ],
      "metadata": {
        "id": "K--kSMFeWpua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering that most pre-trained models normaly have specific input dimensions, it is necessary to set target size such that it matches the input shape of the pre-trained model. "
      ],
      "metadata": {
        "id": "BfPyq_5WlhCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load(what='train', target_size=(224,224)):\n",
        "    array = []\n",
        "    if what =='train':\n",
        "        for file in tqdm(train_files['image'].values):\n",
        "            img = load_img(os.path.join(path, file), target_size=target_size)\n",
        "            img = img_to_array(img)/255. # normalize image tensor\n",
        "            array.append(img)\n",
        "    elif what =='test':\n",
        "        for file in tqdm(test_files['image'].values):\n",
        "            img = load_img(os.path.join(path, file), target_size=target_size)\n",
        "            img = img_to_array(img)/255. # normalize image tensor\n",
        "            array.append(img)\n",
        "    gc.collect()\n",
        "    return np.asarray(array)"
      ],
      "metadata": {
        "trusted": true,
        "id": "eCKvY7ewWpuc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that that has been done, let us load the train and test data"
      ],
      "metadata": {
        "id": "nbxeCYZll6R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Train and Test\n",
        "X_train = load()\n",
        "test = load('test')\n",
        "\n",
        "print(f'train shape: {X_train.shape}\\n test shape: {test.shape}')\n",
        "print(f'test shape: {test.shape}')"
      ],
      "metadata": {
        "trusted": true,
        "id": "uOeo-rsqWpud",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "e82f165d-84b7-45c6-cbdd-afe9e1e23a1f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 3/6252 [00:00<23:22,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7eb8722db5a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Train and Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'train shape: {X_train.shape}\\n test shape: {test.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a732beabdd32>\u001b[0m in \u001b[0;36mload\u001b[0;34m(what, target_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwhat\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.\u001b[0m \u001b[0;31m# normalize image tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ship_classifer_Dataset train/images/2900420.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View Some Train Images"
      ],
      "metadata": {
        "id": "68DoR3qpWpue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the top 28 train images\n",
        "plt.figure(figsize=(12,24))\n",
        "\n",
        "for i in range(1,29):\n",
        "    plt.subplot(7,4,i)\n",
        "    plt.title(f'{train_files[\"ship\"].values[i]}')\n",
        "    plt.imshow(X_train[i])\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "fm9Rxz1MWpuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View Some Test Images"
      ],
      "metadata": {
        "id": "trU1IGmAWpug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the top 28 test images\n",
        "plt.figure(figsize=(12,24))\n",
        "\n",
        "for i in range(1,29):\n",
        "    plt.subplot(7,4,i)\n",
        "    plt.imshow(test[i])\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "del test # free up space for training\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Yxu4mwh-Wpug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Images in color and black and white are combined. Old pictures have been used in some of the photographs. Some pictures show smoke or steam rising from smokestacks. Multiple ships can be seen in certain pictures. Some have clouds, while others have a variety of background scenery. Some of them include a lengthwise display, while others feature a ship's front end. There are some low-contrast photos.\n",
        "We must undertake data augmentation to produce a more robust training set for our neural network in order to solve some of these issues, such as grayscale, rotation, noise, etc. This might improve generalization to the testing dataset and, to some extent, prevent the Neural network from overfitting.\n",
        "\n",
        "It is impossible to know how many edge cases there are by just looking at the top 28 ship photos from train and test, but at first sight, train and test seem to be rather uniform. In other words, unless you overfit the model, validation results should correspond to test scores."
      ],
      "metadata": {
        "id": "A69s2zizWpuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each epoch, need to guage model performance using the f1 score. We will create a custom call back fuction to the effect\n",
        "We need to create a custom callbacks function that calculates f1 score after every epoch to gauge model performance."
      ],
      "metadata": {
        "id": "hCMYDR3rWpui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class printf1(callbacks.Callback):\n",
        "    def __init__(self, X_train, y_train):\n",
        "        super(printf1, self).__init__()\n",
        "        self.bestf1 = 0\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        pred = np.argmax(self.model.predict(np.array(self.X_train)), axis=1)\n",
        "        f1 = f1_score(np.argmax(self.y_train, axis=1), pred, average='weighted')\n",
        "        print(\"Train F1 Score: {:.4f}\".format(f1))\n",
        "        pred = np.argmax(self.model.predict(self.validation_data[0]), axis=1)\n",
        "        f1 = f1_score(np.argmax(self.validation_data[1], axis=1), pred, average='weighted')\n",
        "        print(\"Valid F1 Score: {:.4f}\".format(f1))\n",
        "        return"
      ],
      "metadata": {
        "trusted": true,
        "id": "AxyjWEskWpui"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to plot training/validation history object\n",
        "def plt_dynamic(x, vy, ty, ax, colors=['b'], title=''):\n",
        "    ax.plot(x, vy, 'b', label='Validation Loss')\n",
        "    ax.plot(x, ty, 'r', label='Train Loss')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.title(title)\n",
        "    fig.canvas.draw()\n",
        "    plt.show()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "LW6otaF_Wpuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Greens):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6,6))\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=0)\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Xi3uIqu9Wpuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pre-trained network and weights (Xception)\n",
        "Utilize Keras pre-trained model and weights for faster and more accurate image classification models. Weights were loaded from training on Imagenet dataset."
      ],
      "metadata": {
        "id": "Mz_VFPbbWpuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure internet is enabled in the settings tab to the right\n",
        "# do not include the last Fully Connected(FC) layer\n",
        "from keras.applications.xception import Xception\n",
        "model = Xception(include_top=False, input_shape=(224,224,3))"
      ],
      "metadata": {
        "trusted": true,
        "id": "FI-EEkXEWpul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "\n",
        "x = GlobalAveragePooling2D()(model.output)\n",
        "#x = Dense(6, activation='relu')(x)\n",
        "output = Dense(5, activation='softmax')(x)\n",
        "\n",
        "# define new model\n",
        "model = Model(model.inputs, output)\n",
        "model.save('model.hdf5')\n",
        "#model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "zo8j_WTtWpum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the Xception model architecture\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
      ],
      "metadata": {
        "trusted": true,
        "id": "zkoLzcRBWpun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune Entire Pre-trained Xception Model"
      ],
      "metadata": {
        "id": "0xhleuauWpuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, \n",
        "                                                    stratify=y_train,\n",
        "                                                    random_state=2019,\n",
        "                                                    test_size=0.2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "uRz-rKeFWpup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_EPIxp3_Wpup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It was essential to alter (augment) the current image collection using a data generator due to paucity of data given. Care must however be taken as data augmentation could harm or improve model performance. Consideration of data augmentation must be done case-by-case. This means that adding a vertical flip when none of the ships are turned upside-down will probably make the model less generalizable to test data.\n"
      ],
      "metadata": {
        "id": "YaK3Te2eWpuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use ImageDataGenerator to augment training data\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "batch_size = 8\n",
        "epochs = 50\n",
        "\n",
        "# make sure to keep learning rate low when fine-tuning\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],\n",
        "              optimizer=adam(lr=0.0001))\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=45, \n",
        "                             horizontal_flip=True, \n",
        "                             width_shift_range=0.5, \n",
        "                             height_shift_range=0.5, \n",
        "                             dtype='float32')\n",
        "\n",
        "datagen.fit(X_train, augment=True, rounds=1, seed=2019)\n",
        "train_generator = datagen.flow(X_train, y_train, \n",
        "                               batch_size=batch_size, \n",
        "                               seed=2019)\n",
        "\n",
        "f1 = printf1(X_train, y_train)\n",
        "cp = ModelCheckpoint('best.hdf5', monitor='val_loss', save_best_only=True)\n",
        "annealer = LearningRateScheduler(lambda x: 1e-4 * 0.95 ** x)\n",
        "\n",
        "history = model.fit_generator(generator=train_generator, \n",
        "                              steps_per_epoch=len(X_train)/batch_size, \n",
        "                              validation_data=[X_test, y_test], \n",
        "                              callbacks=[cp,f1,annealer],\n",
        "                              epochs=epochs)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wDnngHh0Wpuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printout competition metric - F1 score \n",
        "true = np.argmax(y_test, axis=1)\n",
        "best = load_model('best.hdf5')\n",
        "valid_pred_best = np.argmax(best.predict(X_test), axis=1)\n",
        "best_f1_score = f1_score(true, valid_pred_best, average=\"weighted\")\n",
        "print(f'Best model weighted F1 Score: {best_f1_score:.4f}')\n",
        "\n",
        "valid_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "f1_score = f1_score(true, valid_pred, average=\"weighted\")\n",
        "print(f'weighted F1 Score: {f1_score:.4f}')\n",
        "\n",
        "# visualize training loss\n",
        "fig, ax = plt.subplots(1,1)\n",
        "vy = history.history['val_loss']\n",
        "ty = history.history['loss']\n",
        "ax.set_xlabel('Epoch')\n",
        "x = list(range(1,epochs+1))\n",
        "ax.set_ylabel('Categorical Crossentropy Loss')\n",
        "plt_dynamic(x,vy,ty,ax, title='Training History - Xception')\n",
        "\n",
        "#plot confusion matrix\n",
        "plot_confusion_matrix(true, valid_pred, normalize=True, \n",
        "                      classes=labels, title='Confusion Matrix')"
      ],
      "metadata": {
        "trusted": true,
        "id": "2Wd1uoq8Wpur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "From the confusion matrix here, it seems that our network is mixing up Cargoes and Tankers. There is also a bit of overfitting - even though this does not seem large enough to hurt our predictions. "
      ],
      "metadata": {
        "id": "LTt2kI4iWpus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Test Predictions"
      ],
      "metadata": {
        "id": "FPjxKM-FWpus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = load('test')\n",
        "sub = pd.read_csv('../input/sample_submission_ns2btKE.csv')\n",
        "\n",
        "# use the better performing model\n",
        "if best_f1_score >= f1_score:\n",
        "    sub['category'] = np.argmax(best.predict(test), axis=1) + 1\n",
        "else:\n",
        "    sub['category'] = np.argmax(model.predict(test), axis=1) + 1 \n",
        "sub.to_csv('submission.csv', index=False)\n",
        "sub.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "tYYkRkG8Wput"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub['category'].map(ship).value_counts(normalize=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "MkSVkT0iWpuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
